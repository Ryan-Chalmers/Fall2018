= ECSE 437

== Overview

=== Reading: Modern Release Engineering in a Nutshell

* 6 major phases of a release pipeline

==== Integration Branching and Merging

* Movement of code changes made on development up to master
* A branch is made as a revision of some parent, has a record of chronicle
sequence of changes (commits) and is eventually merged back into the parent
* Quality assurance activities (code reviews) should be down before merge
* Branch merges can create merge conflicts of different types (text-level,
build-level, test-level, semantic level)
* Rules of thumb to mitigate merge conflicts
** Keep branch merges short-lived and merge often
** Rebase (synch up) with recent changes of the parent branch

* Trunk Based Development: eliminate branches, everyone works off master, thus
eliminating the problem of merge conflicts
* Feature Toggle: Allow the source code of incomplete features to be put inside
a conditional block that is controlled by means of a variable

==== Continuous Integration: Building and Testing

* CI: refers to the activity of
** Continuously polling the VCS for new commits and merges
** Checking these revisions out on dedicated build machines, compiling them
** Running an initial set of tests to check for regressions

* Allows for fast feedback to developer or team while info is fresh in their
mind
* Test stages obtain deliverables built by CI process from "artifact repository"
which is a file server that cant tag files with metadata

==== Build System

* Set of build specification files used by the CI infrastructure to generate
project deliverables from source code
* Consists of two layers
** Configuration Layer: used to select features to be compiled and included
in the resulting deliverables
** Construction: used to specify build tool invocations that generate deliverables
from source code

* Different type of build systems:
** File Level: Make
** Task Based: Ant
** Lifecycle Based: Maven
** High level: cmake

==== Infrastructure as Code

* Refers to server, cloud, container or virtual machine on which a new version
of the system should be deployed for testing or production
* Used to generate the right environment based on a specification developed
in a dedicated programming language
* Virtual machines: self contained systems containing and operating system, and
library or services required to emulate a server that can run the application
under development
* Container: lightweight alternative to a virtual machine
** Share as many components as possible in the form of images
** Save disk space and memory

==== Deployment

* Deployment is the phase in which the tested variables for the upcoming
release are staged in preparation for release
* Dark Launching: deploying new features without releasing them to the public
** The system automatically make calls to hidden features in a way invisible
to end users

* Blue/Green Deployment: deploys the next version on a copy of the production
environment, updates DNS to redirect traffic to new environment
* Canary Deployment: release of the software system is loaded to a subset of
the production environments
* A/B Testing: deploys alternative A of a feature to the environment of a
subset of the user base, while alternative B is released to another subset
** Check which is better

==== Release

* "Rolling Forward": rolling back a release is difficult, instead roll out
a new release to fix the problem

=== Release Pipelines

1. Integrate: VCS
2. Build
* 1. Configuration
* 2. Construction
* 3. Certification
* 4. Packaging
* 5. Deployment

3. Deploy
4. Monitor

== Version Control

=== Git: Distributed Version Control System

* Offline Accessibility:
** Centralized VCS clients need server connections for most operators
** By contrast most operations can be performed on the DVCS clients without
using the server

* Branch and Merge operations
** Branches in CVCSs must be created and merged on the server
** DVCS clients can create and merge branches as lightweight operations on their
client machines

==== Rebasing

* Rebase before merging, automatically rewinds your branch changes

image::images/lecturenotes-c5267.png[align=center]

image::images/lecturenotes-2e4e5.png[align=center]

* Reapply the changes to the new head of the target branch

image::images/lecturenotes-645bd.png[align=center]

image::images/lecturenotes-ffea7.png[align=center]

* After rebasing, the merge option is trivial

image::images/lecturenotes-41342.png[align=center]

==== Overview of Git Architecture

* Decentralized by design
** Each repository can act as a server or a client
** By convention, a server side only repository is typically a bare repository
named with the .git extension

* Git can be served over several protocols
** Local: fasted and most secure
** Git Protocol: slow and bad
** HTTPS and SSH: about same speed and security\

== Build Systems

=== Using C

* Have a group of uncompiled c code *.c
* Then need to compile all the c code: gcc -c *.c
* Then need to link all the dependencies to make program gcc *.o -o program_name
* When we fix a bug in one of the source code files the file must be
rebuilt and then everything thing that depends on it must also be rebuilt
* Tedious

=== Reading: A Large-Scale Empirical Study of the Relationship between Build Technology and Maintenance

* Build System: refers to specification that outline how a software system is
assembled from its sources
* Build Process: the act of assembling a software system, consists of 4 steps
** Configuration step: a set of build tools and features are selected
** Construction step: compiler and other commands that produce deliverables are
executed in an order to satisfy dependencies
** Certification step: automatically execute tests to ensure that produces
deliverables have not regressed
** Packaging step: bundles certified deliverables together with required
libraries, documentation and data files

==== Low-Level

* Explicitly define build dependencies between input and output files, as well
as the commands that implement the input-output transformation
* Make: targets specify build files created by a recipe, i.e. a shell script
that is executed when target either does not exist or is older than one
of its dependencies
* Phony Target: represents abstract phases of a build process rather than
concrete files in a filesystem

==== Abstraction Based

* Maintainers have to constantly repeat low-level expressions to account for
variability in platform, abstraction fixes this
* Automatically generate low-level specifications based on higher level
abstractions

==== Framework driven

* Framework-driven technologies favor build convention over configuration
* Maven

==== Dependency Management

* Augment build systems by managing external dependencies, making sure the local
cache contains all APIs necessary to build the project
* Offer two advantages
1. users no longer need to carefully install required versions of libraries
manually
2. production and development environments can coexist, since the potentially
unstable versions of libraries that are required for development are placed
in the local cache that is quarantined from the running system

=== Low Level Build Systems

==== Make

1. Expressing Dependencies
** Program
*** random.o , input.o, main.o
**** random.c, input.c main.c

2. Writing Recipes
----
program : random.o input.o main.o
  gcc -o program random.o input.o main.o

random.o : random.c
  gcc -c random.c

my_command="gcc -c input.c"
input.o : input.c
  $(my_command)

main.o : main.c ; gcc -c main.c

.PHONY: clean
clean:
  rm -f random.o input.o main.o program
----

* Equivalent to above *Good to know, Midterm*
----
program: random.o input.o main.o
  gcc -c $@ $^
%.o
  gcc -c $<

.PHONY: clean
clean:
  rm -f random.o input.o main.o program
----

===== How does make work?

image::images/lecturenotes-dee63.png[align=center]

image::images/lecturenotes-166a8.png[align=center]

image::images/lecturenotes-c7fca.png[align=center]

===== Shortcomings with low-level build tools like make

* Platform specific scripts: not portable, repeated lines
* Recursion based build design is harmful
** to handle dependencies across directories, recursive calls to make are used,
this fractures the global dependency graph

=== Abstraction Based Build Systems

* Addresses shortcomings of low-level specs

* GNU Autotools
** Expands macros to generate platform specific Makefiles and production code

* CMake:
** A toolchain that again expands higher level abstractions to generate lower
level build specs
** Generates Makefiles, MVS and xcode files
** Exercise Using CMAKE from lecture: *Lab Tutorial*


=== Framework Driven Build System

==== Maven

* Maven Build Process
** Maven assumes that a build job follows a lifecycle
** A lifecycle is a sequential series of phases
** A phase performs a series of goals that are bound via plugins

* Maven build lifecycles
** Default: produces the project deliverables
** Clean: undoes build commands to return the project to its initial state
** Site: generates the project website materials

image::images/lecturenotes-fb9af.png[align=center]

===== Goal Binding for Maven Lifecycles

* Phase to goal bindings
** What is performed during a phase depends on the goals that are bound to it,
and the plugins that are associated with those goals
** A phase with no goals or plugins is a noop

* The bindings depend on the deliverable
** The deliverable is what maven is tasked with producing
** The goals that are bound to lifecycle phases by default depend on the
deliverable that we are trying to produce

=== Dependency Management Tools

* Internal dependencies become easier to track
* Projects are becoming more and more connected
* Maven manages repositories of dependencies on your machine
* Maven Central: an online resource where packages are published and made
available to Maven users

==== Maven as an External Dependency tool

* Dependency resolution:
** Dependencies are expressed in the pom.xml file
** If an appropriate version of a dependency is not available in your local repo,
it is downloaded from maven central

* Dependency Scope:
** Dependencies can be bound to specific phases, scoping their impact down to
those users who run those phases

== Routine Builds and Continuous Integration

=== Build system Interactions

==== Developers

* Developers execute personal builds on their development machines to:
** Synch their changes into deliverables so that they can perform basic tests
** Ensure that their changes have not introduced build problems

==== Quality Assurance

* QA teams hook automated tests into the build system
* Since normal builds should proved quick feedback only quick tests are run
by defaults
* Slower test can be relegated to special build targets

==== Release Engineers

* Release engineers are concerned with micro and macro aspects of build systems
** Micro-build: concerns about the behavior of a build system within a single
execution
** Macro-build: concerns about how to best provision a fleet of build resources

==== Static Analysis

* Static analysis tools listen to commands that are executed by the build system
* Use executed commands to put together a graph of how source code files are
connected
* Scan the graph for common bugs like resource leaks and dead code

==== Code Review Environments

* Teams that use modern code review tools like Gerrit connect build hobs to
the code reviewing dashboards

==== Nightly Builds

* At night, a build is executed to produce deliverables that include the changes
of the prior day
* QA teams can pick up those deliverables in the morning to test the new feature
and verify bug fixes
* Issues:
** As the amount of change per day has grown, nightly builds have become
difficult
** If there is build commits on the build then it is hard to tell which commit
caused the break

* Solution: run frequent smaller builds

=== Continuous Builds

.Continuous Integration Feedback loop
image::images/lecturenotes-17154.png[align=center]

* Each push gets its own build job
* CI simplifies problem assignment: problems correspond to a logical set of
changes made by one developer
* CI simplifies problem analysis: reports errors quickly

=== Reading: Modern CI Process

==== Build Triggering Events

* Cycle begins with build triggering event
* Builds are triggered:
** During development
** When the code is submitted for review
** The change is integrated into the project VCS

==== Build Job Creation Service

* When a build triggering event occurs, a build job creations node will add
a job to the queue of pending build jobs

==== Build Job Processing Service

* build jobs in the pending queue will be allocated to a build job processing
node
* The job processing node will first download the latest version of the source
code and apply the change under consideration
* Next the job processing node will initiate the build process, which will
compile the system if necessary, execute a system of automated tests, and make
the system available for users

==== Build Job Reporting Service

* build jobs results in the reporting queue will be communicated to the
development team
* Reporting preferences can be configured such that particular receipts receive
notifications when builds are successful

=== Keeping Build Systems Robust

* Build systems need maintenance
** As source code changes, occasionally the build system will need to be updated
** Neglected build maintenance makes the build process produce incorrect
results

=== Test Flakiness

* Flaky Tests: a test with non-deterministic behavior
* False positives: test code has a failed test when the code is actually fine
* False negatives: test passes when it should have failed

image::images/lecturenotes-17154.png[align=center]

== Code Inspections and Code Review

* Use knowledge of the system, its domain and the technologies used to discover
problems

=== Software inspections and Review: Advantages over dynamic V&V

* Cascading errors can obfuscate test results: Once an error occurs, later
errors may be new, or are the cascading effect of prior error
* Incomplete versions can be inspected: tests require an executable version of
the system, while inspections do not
* Good inspections are more than "Bug Hunts"
** Inspection uncover inefficiencies and style issues
** Inspection are a form of knowledge sharing and collaborative problem solving

=== Structured Inspections

* Rigid heavyweight process
* Involve in-person meetings, review checklists

* Roles
** Moderator
** Scribe
** Reviewer
** Reader
** Producer

.Structured Inspection
image::images/lecturenotes-a91a3.png[align=center]

=== Tips for a Productive Code Review

[options="header"]
|==================
|Do: | Dont:
| Critique the artifact | Attack the person
| Keep review chunks short and succinct | submit fixes for multiple issues at the same time
| Plan time for review | skip reviews
| Prioritize review of important issues | FIFO queue reviews
| Keep things light | Use sarcasm or exaggerations
|==================

=== Code Review Manifesto

We are uncovering better ways of critiquing code by doing it and helping others
to do it. Trough this work we have come to value +
Constructive Feedback _over_ Ad hominem attacks +
Clearly articulated rationale _over_ strongly worded opinion +
Suggested means for improvement _over_ only focussing on the criticism +

=== Modern Reviews

==== Reading: Why do programmers do code reviews?

===== Finding Defects

The top listed reason for doing code reviews. Both low-level (correct logic is
  in place) and high-level (catch errors in design).

===== Code Improvement

Comments or changes in the code in terms of reliability, commenting, consistency,
dead code removal , etc but do not involve correctness or defects.

===== Alternative Solutions

Regard changes and comments on improving submitted code by adopting an idea
that leads to a better implementation.

===== Knowledge Transfer

One of the main purposes for a code review is distribution of knowledge, if you
did not learn anything from a code review then it wasn't a very good review.

===== Team Awareness and Transparency

Code reviews act as a signaling mechanism to notify other devs that there are
changes being made in the the code base and that if they would like, they can
observe these changes.

===== Share code ownership

Breaks the idea that each person owns their code and instead code was writte
so the team can progress as a whole.

==== Roles

* Author
* Reviewer
* Integrator
* Verifier

==== Gerrit

image::images/lecturenotes-4ebb1.png[align=center]

== Midterm Questions

* Describe a file based build system (ie. Make)
* What is a container and what are its benefits over a VM?
* What is trunk-based development and how does it reduce the burden of branches
merges and conflicts
* what is a phony target/example?
* what are the four steps of a build system
* Why do companies prefer rolling forward over rollbacks
* Give 1 advantage and 1 disadvantage of GitFlow over trunk-based development
* What is trunk based development and how does it reduce the burden of branches
merges and conflicts
* What is a container and what are its benefits over a virtual machine

== Cloud-based Deployment

=== What is the Cloud

* Shared pools of configurable computing resources
* Can be rapidly provisioned (semi-)automatically
* Can "elastically" scale based on demeand

=== Business Case for Cloud Computing

* For cloud computing: cloud providers automatically scale applications based
on demand

.Business Case for Cloud Computing
image::images/lecturenotes-e1fe4.png[align=center]

* Business case against cloud computing: you are using someone else's computer,
security, privacy and other concerns about IP

=== Infrastructure as a Service (IaaS)

* A service that provides computing resources to users
* Typically, the computing resources are in the form of virtual machines or
containers

* Business Models
** Compute hours/month
** Per-machine, tshrt sizing

* Examples:
** Linode
** Amazon EC2
** Azure
** Google Cloud Platform
** IBM Cloud
** Digital Ocean

==== IaaS vs In-house Infrastructure

* Pros:
** Explainable infrastructure operation costs
** IT support is offloaded to cloud provider
** Elastic scaling

* Cons:
** Data privacy and security may be difficult to verify
** Costs can grow in unexpected ways

===== EC2 Exercise

* Using EC2 we created an instance running Red Hat Linux or Ubuntu
* Once the instance is instantiated we can connect via SSH
* We used yum http to install apache on the server and created a basic html
page
* We had to configure the security group settings to allow http connections

=== Platform as a Service (PaaS)

* A service that provides a development platform to users
* The platform provides users with a comfortable application development entry
point
* Exposes configuration options relevant for devs while hiding irrelevant options

* Business Models
** Wall clock hours of availability per month
** Per-machine t-shirt sizing

* Examples
** Heroku
** Azure
** Cloud Foundry
** IBM Cloud
** Google Cloud Platform

==== PaaS vs. IaaS

* Pros:
** Devs can focus on app development instead of infrastructure config
** Platforms are typically tuned to near-optimal settings for most application
workloads

* Cons:
** Convenience at the cost of configurability
** Platform lock-in: hard to port applications to other platforms

===== Heroku Exercise

* Download and install heroku
* Login to heroku using _heroku login_
* Downloaded an instance of getting started application
* create a new app in the current working directory (main directory of the web
application) by running _heroku create_
** Doing this creates a new Git remote called heroku
** Pushing to this new remote initiates the deployment process

* Initiated a deployment by pushing _git push heroku master_
* Scale heroku instances to ensure at least one instance is running:
_heroku ps:scale web=1_
* Check application logs by using _heroku logs_ or use the _--tail_ option to
see them update live
* *Procfile*: defines how the application is launched
** web: a special processs type that connects to Heroku's HTTP routing stack:
_web: java -jar target/java-getting-started-1.0.jar_


=== Software as a Service (SaaS)

* The entire stack (infrastructure, platform, execution environment) is hosted
in the cloud and delivered over the web
* Usually involves a so-called "thin client" that connects to a backend that
delivers most of the functionality

* Business Models:
** Public user plans (typically free)
** Business user plans (monthly rate on usage capactity)

* Examples
** GSuite (goolge docs etc.)

=== Mobile Backend as a Service (MBaaS)

* Many application backends provide similar features
** Identity management
** Notification routing
** Social media integration
** Cloud-like storage

* Rather than "reinvent the wheel", providers share these features as APIs

* Business Models:
** Freemium Models
** Up to a certain number of active users per month are provided for free
** More active user is more money

* Examples
** Backendless
** Firebase

==== Firebase Authentication

* Provides backend services, easy to use SDKs, ready made UI libraries to
authenticate users
* Saves users in the cloud allowing identity to be used accoss all applications
* FirebaseUI: a complete drop in solution for your application, simply paste the
code in an its there
* First authenticate credentials from the user, send these to the backend to
verify those credentials, then sends a response back to the client, if all is
a success you will have basic profile information available


=== Serverless

* Serverless is a of variant of IaaS where capacity planning decisions are made
by the provider
* Costumers are charged for the resources their applications use
* Elastic scalability at the application level rather than the virtual machine
level

* Business Models:
** Similar to IaaS except charges are calculated based on the resouces that
your application requires

==== Serverless vs. IaaS

* Pros:
** Users pay for only the compute resources their code consumes
** Code can be deployed can scaled on a finer granularity

* Cons:
** A little complex to learn
** Costs can be a little tough to predict

=== Function as a Service (FaaS)

* Primary means by which serverless computing is realized today
* Functions are deployed to be executed on demand within a cloud environment

* Business Models:
** Freemium style

==== AWS Lambda Getting started Tutorial

* From the AWS management console author a new function from scratch
* Give the function a name, runtime, role
* There is an option set up triggers for your function
* Other optional config options: environment variables, tags, execution roles,
basic settings, network, debugging and error handling, concurrency, auditing
and compliance
* Events: represented by key value pairs in a json


== Virtualization

* Virtual Machines
** Emulate computer system
** Provide a computer system from within the scope of another system

* Makes entire machines "shippable"
* Gives shared infrastructure users the impression of having their own machine

=== Different Types of Virtualization

* System virtualization
** Hosts an entire operating system

* Process Virtualization
** Provides the isolation of system virtualization while also sharing common
software tools with the underlying platform

=== History of Virtualization

image::images/lecturenotes-41f24.png[align=center]

==== '60s

* Early server virtualization
** Goal: run legacy software on newer hardware
** Result: CP-67 system, first commercial mainframe to support virtualization
** Pros: isolation

==== '80s and '90s

* chroot
** Goal: treat a directory within filesystem as another root directory
** Pros: sharing of system and kernel level features

* Java
** Goal: compile-once-run anywhere
** JVM abstracts hardware details

* VMware
** Workstation: run VMs on UNIX/Linux (Type 2)
** GSX Server: run VMs on Windows (Type 2)
** ESX Server: run VMs without host OS (Type 1)

===== Chroot Jail Exercise

* Create a directory called jail and create a bin directory in it
* Copy /bin/bash into your jail/bin directory
* Run ldd to list the shared libraries that we need to execute bash in the jail
* Create a directory called lib64 within the jail and copy the libraries into
there
* run chroot to change root into your jail

===== Hypervisor Types

* AKA Virtual Machine Monitors (VMMs)

image::images/lecturenotes-dc3d7.png[align=center]

==== '00s and '10as

* Containers:
** Initial appearance on solaris 10
** Linux support in 2008 "Zones" allow lightweight isolation with a shared core
of OS, system, and application software
** Containers popularized by Docker, which automates deployment

==== Linux Containers and Docker

* Containers:
** A set of processes, running on top of a common kernel
** Isolated from the rest
** Cannot impact each other on the host
** Achieved through namespaces and cgroups

image::images/lecturenotes-b6606.png[align=center]

=== Docker:

* An orchestration tool for containers with severel features
** Portability
** App-centric
** Builds from "source"
** Versioning
** Component reuse
** Public registry
** Tool Ecosystem

image::images/lecturenotes-48038.png[align=center]

* Docker provided standardized container format
* Fixed inefficient use of CPU resources, allowing for isolated services in fewer
VMs

==== Docker Architecture

* Provides the ability to package and run an application in a loosely isolated
environment called a container
* Lightweight because they don't need the extra load of a hypervisor, but run
directly within the host machines kernel

===== Docker Engine

* client server with the following major components
** A server which is a type of long-running program called a daemon process (the
dockerd command)
** A REST API which specifies interfaces that programs can use tot talk to the
daemon and instruct it what to do
** A CLI client (docker command)

* CLI uses the Docker REST API to control or interact with the Docker daemon
through scripting or CLI commands
* The daemon creates and manages Docker objects, such as images, containers,
networks and volumes

.Docker Overview
image::images/lecturenotes-1958a.png[align=center]

===== Client-server Architecture

.Docker Ecosystem
image::images/lecturenotes-54213.png[align=center]

* Client talks to Docker daemon which does the heavy lifting of building, running,
and distributing the docker containers
* Client and daemon can run on the same system or you can connect to a remote
* Communicate using the rest API
* Docker daemon:
** listens for docker api requests and manages docker objects
** can also communicate with other daemons

* Docker registry stores docker images (docker hub)

===== Docker Objects

* Images
** a read-only template with instructions for creating a Docker container
** images usually based on others (FROM)
** When a docker file is updated only the layers which have changed are rebuilt

* Containers
** A runnable instance of an image

* Services
** Allow you to scale containers across multiple docker daemons, which all work
together as a swarm with multiple managers and workers

====== Underlying Technology

* Namespaces
** used to create isolated workspaces called containers
** Each aspect of a container runs in a separate namespace and its access
is limited to that namespace

* Control Groups
** limits an application to a specific set of resources, allowing docker to
share available hardware resources to containers

* Union File Systems
** file systems that operate by creating layers, making them lightweight and fast


==== Traditional vs Container-based Development

* Traditional Development
** Presume we are developing a Python app
** First, we need a python runtime for your development machine
** The chouce of python runtime imposes constraints on the runtime that can be
used in produciton

* Container-based Development
** Development requirements, execution platform, dependency stack, and application
code all evolve together
** Portable images define execution platfrom
** Defined using Dockerfiles
** Instantiated as containers

==== Docker Exercise

* Downloaded the source code for the software rebels webpafe
* Created a Dockerfile
* Set the parent image to jekyll/jekyll by using: _FROM jekyll/jekyll_
* Set a working dir within the container: _WORKDIR /tmp_
* Copy rebels into working directory: _COPY . /tmp/_
* Set the command to be run on docker run: _CMD ['jekyll','start']_
* Built the image: _docker build_
* Check available images _docker image ls_
* Create an instance of the container, mapping port to port 4000 and running in
the background: _docker run -d -p 4000:4000 <image>_
* Check running containers: _docker container ls_
* Stop container: _docker container stop <id>_


== Infrastructure as Code

=== Operating System Configuration

* Networking
** Network Connection
** Identity services
** Mounting shared storage

* Kernel Features
** Storage disk encryption
** File systems
** Hardware acceleration

=== Middleware and Application Server Configuration

* Spring/Spring Boot
** Bean config
** Component settings

* httpd
** Plugins
** DocumentRoot
** SSL

=== Application Configuration

* Many UNIX application have an rc file:
** By convention these are read at application launch time
** Vim: ~/.vimrc
** Bash: ~/.bashrc, ~/.bash_profile

=== Why is Infrastructure Config a Problem

* Automation is hindered
* If configuration is done by hand, it can become difficult to maintain
* Release engineers and operators spend too much time putting out fires with
configuration details
* This wastes precious resources (human hours)

=== Solution: Infrastructure as Code

* Write your configuration as code
* Manage and provision machines using code-like syntax rather than interactive
configuration tools
* Has all of the advantages of code
** Can be stored and versioned in a VCS
** Can be automatically executed to perform tasks

=== Puppet

* Puppet lets you define a desired state for all systems in your infrastructure
* Puppet automates getting your systems into the desired state
* Puppet runs using a master-agent architecture
* De-centralized: Agent periodically checks the master and updates locally

==== Resource Syntax

image::images/lecturenotes-2079a.png[align=center]

* type: the kind of thing the resource describes
* Body: is a resource list of parameter value pairs (parameter => value)
* title: a unique name that puppet uses to identify the resource internally
** You can set the resource title to something other than its path if you set
the path parameter to its path

* ensure parameter: tells us the basic state of the resource
* Provider: is the translation layer that sits between Puppet's resource
representation and the native system tools it uses to discover and modify the
underlying system state
* puppet resource: allows you to see the system the way puppet does, through
the resource abstraction layer

==== Agent Master

image::images/lecturenotes-bf97d.png[align=center]

* Agent begins Puppet run by sending a catalog request to master (Facter)
* Master uses information from catalog and puppet code to tell the agent
how the resource code should be configured
* Agents must present a certified signed certificate to communicate with master
* On *sudo puppet agent -t* (puppet run):
** plugins are copied from master to agent to ensure all tools are available

==== Classification

* Node classification: determine what puppet code will be compiled to generate
a catalog for agent, triggers when the master receives a catalog request with
valid certificate
* Three ways to handle node classification
1. site.pp manifest
2. PE console node classifier GUI
3. External node classifier

==== Classes and Modules

* A class is named block of Puppet code
* Defining a class combines a group of resources into single reusable and
configurable unit
* Once defined, a class can then be declared to tell Puppet to apply the
resources it contains
* A module is a directory structure that lets Puppet keep track of where to find
the manifests that contain your classes.

== Midterm Questions

* Cloud types of the following technologies
** Heroku: PaaS
** Amazon EC2: IaaS
** Firebase: MBaaS
** IaaS

* Type 1, Type 2 and Container based deployment environments

.Type 1
image::images/lecturenotes-c50aa.png[align=center]

.Type 2
image::images/lecturenotes-7a045.png[align=center]

.Type 3
image::images/lecturenotes-50da1.png[align=center]

* Key Difference between a type 1 and type 2 hypervisor
** A type 1 hypervisor has no OS running on he host system, a type 2 hypervisor
does have an OS

* How does container-based development differ from traditional development
** Container based development ships complete and runnable bundles
** Containers are portable
** Containers contain the infrastructure required for the application

* What is serverless and how is it typically achieved?
** Serverless is a variant of IaaS where capacity planning decisions are made by
the provider
** Serverless is elastically scalable at the application level
** This is usually achieved by function as a service
** Functions are deployed to be executed in a cloud environment and owner is
charged by number of function calls

* Dockerfile
----
FROM rikorose/gcc-cmake
WORKINGDIR /tmp/
COPY . /tmp/

CMD ["cmake"]
CMD ["make"]
----

* What is the purpose of virtualization? what are two key types?
** VM, and Container
** The purpose of virtualization is two allow one machine to run multiple
instances of an operating system

* Which Docker command do we need to use to transform a Dockerfile into its
image?
** docker build

* Which sub command do we use to view the shared libraries that an executable
depends on?
** ldd <executable>

* How does a technology like docker differ from a technology like puppet?
** Docker comes pre-assembled with all the necessary requirements, ie. correct
version of python installed, correct OS etc.
** Puppet maintains the appropriate configs by writing the infrastructure as code
and maintaining the infrastructure by referencing this code

* Draw a diagram to illustrate how the docker client, daemon, and registry
communicate when the docker build subcommand is run on the below Dockerfile
----
FROM jekyll/Jekyll
WORKDIR /app
COPY . /app
CMD ['jekyll', 'serve']
----

** Daemon communicates with registry to retrieve the jekyll image
** Not sure...

* After deploying a web application to an IaaS cloud provider and starting the
web server, when we point out web browser to the correct URL the page is
not displayed. Reason?
** Need to allow http access in the security group settings

== Deployment Strategies for Large Userbases

=== What is Software Development

* Releasing Strategies
** Shipment media
** Uploading a tarball to a location on the web
** Push changes into packaging repositories like Maven Central or home brew
** Upload to the app store
** Replace the previous version of application resources on web servers

* Software releases of the past were
** Rare
** Large
** Expensive to produce
** Intrusive to install

* Modern software releases are
** Frequent
** Compact
** Inexpensive to produce
** Non-intrusive to install

=== Release Examples

* Bluth Banana POS Software
** Developer writes code on his local machine
** Builds the code using maven
** Build pushed some artifact to a company server
** POS software machine pings server checking for update
** Machine user can choose to update if there is an available update
** Update is install over encrypted connection

=== Blue-Green Deployment

==== What Challenge does blue-green deployment tackle?

* Downtime during cut over:
** One of the key challenges with maintaining a deployment environment is cutover
*** When a release candidate is promoted from testing to production environment

** Blue-green deployment has a router thats routing all traffic to the production
site
** When it wants to switch over to the new deployment the router simply points
all incoming requests to the new production environment

image::images/lecturenotes-8e160.png[align=center]

* Failed Promotions:
** In the case of a failed promotion the requests were sent to the broken version
are at risk of going unfulfilled
** To mitigate this risk send requests to both blue and green deployments during
cutover, if the new environment fails, the previously running environment can
resume running (*Soft Switchover*)

==== Disasters

* Catastrophic failure of hardware or software components needed to deliver a
service
* Disaster Prevention:
** Design and deploy systems in such a way that disasters cannot happen
** Very difficult in practice

* Disaster readiness
** Expect the unexpected
** Design and deploy systems in such a way that should a disaster occur, the
system can quickly recover

* Disaster Recovery
** Ideally a readiness plan is automated
** Should a disaster occur in the deployment environment, a spare environment
shoyld be able to take over

==== Blue-Green and Disaster Recovery

* Environmental redundancy
** Blue green deployments are essentially disaster recovery
** If a failure occurs while the blue deployment is live, the green deployment
can take over

* Disaster recover should be tested
** we want to make sure that when a problem occurs, the emergency procedures work

==== Three Tier Architecture

image::images/lecturenotes-12dac.png[align=center]

* Database schemas evolve
** As the system changes, the database also needs to be reorganized
** The schema changes may make the previous logic layer obsolete

* Perform Schema updates in Two Phases
** Phase 1
*** First, the database schema should be refactored in such a way that both
new and old logic layers work
*** This way if promotion fails the application can fail back over to the prior
deployment

** Phase 2
*** After the analytics about the new deployment are clean, the old schema
details can be retired

=== A/B Testing

* Experimental design:
** Subjects split into two groups
** Control: do not receive any treatment and are included to measure baseline
behavior
** Treatment: receive a treatment for which a hypothesis has been formulated

* Randomized Experiments
** Subjects are randomly assigned to the control group or one treatment group
** Each participant should have the same likelihood of appearing in either group

* A/B testing is the application of randomized experiments to software testing
* A/B testing implies that, at its core, a software change can be formulated
as a hypothesis

* Hypothesis formulation
** In the context of software, hypothesis can be formulated about changing/improving
some "bottom line" related measurements
*** Click through rate
*** Performance measurements

** E.g.: Changing the button colour from blue to green will increase the click
through rate

* Group Assignment
** Randomly split the population of users into two groups of equal size: A and B
** Each group is shown a system variant, where A is usually shown the control
and B the treatment version
** Each user has a n equal probability of being assigned to group A or B

=== How do we Know if the Experiment was Successful

.https://upload.wikimedia.org/wikipedia/commons/2/2e/A-B_testing_example.png
image::images/lecturenotes-71794.png[align=center]

* Statistical hypothesis testing:
** Used to check if two samples are derived from the same distribution
** The output of a statistical test can usually be represented using a p-value
*** The likelihood of two samples coming from the same population

** To perform the hypothesis test, the p-value is compared to a threshold (alpha)
value
** If the p-value is less than alpha, the null hypothesis should be rejected

=== Lady Tasting Tea Example

* Null Hypothesis
** The lady has no ability to detect the tea with milk-first

* Experiment
** N=8 cups of tea
** Four milk first, four milk after
** She labels cups milk-first or not

.Number of possible combinations
image::images/lecturenotes-8bf04.png[align=center]

* Distribution of possible outcomes

image::images/lecturenotes-d43d0.png[align=center]

* If she choses all 4 cups correct then we can conclude that she has the ability
to detect the cups

=== Fischer Exact Test

.Contingency Table
image::images/lecturenotes-bd175.png[]

=== Canary Relases

* The Problem:
** Each software release introduces some risk
** How can we minimize risk of deploying broken releases to global userbase

* Working definition
** Partial, time-limited deployment of a change in a service
** Followed by an evaluation of the safety of the changed service
** Production may then
*** Roll forward
*** Roll back
*** Alert an operator

==== Canary Analysis Service (CAS) at Google

image::images/lecturenotes-1fe92.png[align=center]

. Set up Canary
.. Rollout tool exposes canary to a subset of the user population
.. Remainder of population serves as control
.. CAS does not help with population selection
. Ask CAS
.. Rollout tools "ask" CAS to evaluate the canary
.. Results are binary PASS/FAIL
.. Results are derived from official statistical tests or commonly adopted
heuristics
. Receive Verdict
.. Each request to CAS executes a trial
... A series of requested checks
.. Result is PASS iff all checks pass, FAIL otherwise
. Roll Forward/back
.. The rollout tool can make a decision based on verdict
... if Pass, then rollout
... if Fail, then roll back
.. The rollout tool may also report results to devs

==== How Does CAD Look Internally

image::images/lecturenotes-0b6a5.png[align=center]

* RPC Interface
** Evaluate(): returns an EvaluationID (Unique URL); non blocking call
** GetResult(EvaluationID): Blocks until evaluation ID is actually complete

* User configuration
** For each check:
*** What is its name?
*** How to get a time series for the metric
*** How to transform time series into verdict

* Monarch
** An external monitoring service
** User-expressed queries are rewritten to independently analyze control population

TODO:: Reading

=== Chaos Engineering

* The problem:
** Production conditions are hard to predict
** Rarely occurring field conditions may lead to the catastrophic failures in the
field

==== What We would Like to do
* Examples of hard-to-test-for failures
** Improper fallback settings when a service is unavailable
** Retry storms from improperly tuned timouts
** Outages when a downstream dependency receives too much traffic
** Cascading failures when a single point of failure crashes


* Early Identification
** We should test systems for sensitivity to change in deployment environments
** Passing such tests would increase teams confidence in releasing products
to chaotic deployment environments

* Understanding Chaos
** Testing all possible environment conditions suffers from a combinatorial
explosion
** Instead, aim to mimic the chaos of real world deployment environment

==== Solution: Chaos Engineering

* Mimicing realistic deployment environments
** Since realistic scenarios are hard to imagine, one can think of deployment
environments as chaotic
** The degree to which a system can withstand chaotic deployment changes can
help us gain confidence in the resiliency of a release

* Formulating the Experiment
** Define a study state deployment setting
*** Express how steady state execution and output can be checked using metrics

** Introduce variables to express real-world events
*** Server crashes, hard disk failures, cleaning staff knocks network cable
out of server

* Null Hypothesis
** A real-world event has no impact on execution or output metrics

* The experiment
** Characterize the steady state by taking measurements in production
** Simulate real-world event
** Characterize the post-treatment environment by taking measurements in production
** Restore the previous production setting

* The analysis
** Apply a statistical hypothesis test or a heuristic-based analysis to check if
the null hypothesis should be rejected

* Lather, Rinse, repeat
** Repeat the experiment for other real-world events
** Use randomness to decide which real-world events to simulate

TODO:: Reading
